{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c272574-6a76-40c2-b7fd-9cadce7ef1c5",
   "metadata": {},
   "source": [
    "## Unsloth: Optimizing Training and Inference Performance\n",
    "\n",
    "For many software algorithms, the performance does not only depend on the number and kind of calculations performed. Instead, the exact order and the size of chunks has an enormous influence on the calculation speed.\n",
    "For large language models, a library called `unsloth` contains optimized GPU kernels created by manually deriving all compute heavy math steps. By using these optimized kernels, a significant speed-up can be obtained.\n",
    "\n",
    "### Key Techniques in Unsloth:\n",
    "\n",
    "1. **Efficient Data Loading**: Optimizing data pipelines to reduce latency and improve throughput during training.\n",
    "2. **Batching and Padding Strategies**: Dynamically adjusting batch sizes and minimizing padding to optimize memory usage.\n",
    "3. **Half-Precision and Quantized Inference**: Using mixed precision or quantized models to speed up inference and reduce memory footprint.\n",
    "4. **Model Pruning and Distillation**: Reducing the size of the model by removing redundant parameters or training smaller models to mimic larger ones.\n",
    "\n",
    "### Benefits of Unsloth:\n",
    "\n",
    "- **Reduced Training Time**: Optimizing data loading and model architecture reduces the time required for each epoch.\n",
    "- **Lower Memory Usage**: Using techniques like mixed precision and quantization reduces the amount of GPU memory required.\n",
    "- **Faster Inference**: Optimizing the model for deployment can significantly reduce latency during inference.\n",
    "\n",
    "### Hands-On Example: Efficient Data Loading and Mixed Precision Training\n",
    "\n",
    "In this example, we take the example from the previous notebook (\"PEFT\") and adjust them to use `unsloth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ffe0c8-2fc5-4885-8ea1-881beb6b2447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_184776/2490468213.py:10: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n",
      "/home/gkosta/finetuning_event/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1191: SyntaxWarning: invalid escape sequence '\\:'\n",
      "  r\"for ([^\\s]{1,}) in \" + modulelist_item + \"\\:[\\n]\" + \\\n",
      "/home/gkosta/finetuning_event/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1434: SyntaxWarning: invalid escape sequence '\\('\n",
      "  regex_find = f\"{call_class}\\(([^\\)]{{1,}})\\)\"\n",
      "/home/gkosta/finetuning_event/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1434: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  regex_find = f\"{call_class}\\(([^\\)]{{1,}})\\)\"\n",
      "/home/gkosta/finetuning_event/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1440: SyntaxWarning: invalid escape sequence '\\('\n",
      "  regex_find = \"def forward\\(([^\\)]{1,})\\)\"\n",
      "/home/gkosta/finetuning_event/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1573: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  inherited_modules = re.findall(r\"class ([^\\s]{1,})\\(\" + inherited_class + \"\\)\", full_source)\n",
      "/home/gkosta/finetuning_event/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1627: SyntaxWarning: invalid escape sequence '\\('\n",
      "  called = re.findall(r\"[\\s]{1,}\" + re.escape(function) + \"\\(.+?\\)\", full_source, flags = re.DOTALL)\n",
      "/home/gkosta/finetuning_event/.venv/lib/python3.12/site-packages/unsloth_zoo/peft_utils.py:225: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  name = re.sub(\"\\.([\\d]{1,})\\.\", r\"[\\1].\", name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import BitsAndBytesConfig, pipeline, TrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "## Instead of:\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "## use:\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8c4214-75f3-4936-b9be-263dab081a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.6.2: Fast Llama patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.527 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00cedc153e94d8d922384c5c9f42778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a493a3dd1f14264aba84a85a7c3c58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be879fd31f74704b0a586753257ca9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/140 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4a0d053ede4b26809b9a6f1d991e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c944b4aa8db42c2a71f3decd81359eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68383642687448f395ef217c784c379c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd6c8fe08c1480d81b896d60965ce32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcad66f343bd42da80a4c893c4964dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Choose a model and load tokenizer and model (using 4bit quantization):\n",
    "# model_name = \"unsloth--Phi-3.5-mini-instruct-bnb-4bit\"\n",
    "model_name = \"unsloth/Phi-3.5-mini-instruct-bnb-4bit\"\n",
    "# model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "## Instead of:\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(...)\n",
    "## use: \n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir='models',\n",
    "    ## Instead of:\n",
    "    # quantization_config=BitsAndBytesConfig(...)\n",
    "    ## use:\n",
    "    load_in_4bit=True,\n",
    "    # device_map='cuda:0',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.padding_side = 'right'\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b7843e6-8ca3-4309-92a6-358973336018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Load the guanaco dataset\n",
    "guanaco_train = load_dataset(\"timdettmers/openassistant-guanaco\", cache_dir='data', split='train')\n",
    "guanaco_test = load_dataset(\"timdettmers/openassistant-guanaco\", cache_dir='data', split='test')# guanaco_test = load_dataset('/leonardo_scratch/fast/EUHPC_D20_063/huggingface/datasets/timdettmers--openassistant-guanaco', split='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814d0851-8920-465d-9fb3-2c776fcef64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "guanaco_train = guanaco_train.map(lambda entry: {\n",
    "    'question1': entry['text'].split('###')[1].removeprefix(' Human: '),\n",
    "    'answer1': entry['text'].split('###')[2].removeprefix(' Assistant: ')\n",
    "})\n",
    "guanaco_train = guanaco_train.map(lambda entry: {'messages': [\n",
    "    {'role': 'user', 'content': entry['question1']},\n",
    "    {'role': 'assistant', 'content': entry['answer1']}\n",
    "]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65644a2c-2be2-40a8-be2f-ea57e3ef1aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.6.2 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "## Instead of:\n",
    "# peft_config = LoraConfig(\n",
    "#     task_type='CAUSAL_LM',\n",
    "#     r=16,\n",
    "#     lora_alpha=32,  # thumb rule: lora_alpha should be 2*r\n",
    "#     bias='none',\n",
    "#     target_modules='all-linear',\n",
    "# )\n",
    "# model = get_peft_model(model, peft_config)\n",
    "## use:\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=32,  # rule: lora_alpha should be 2*r\n",
    "    lora_dropout=0.05,  # Unsloth supports any, but = 0 is optimized\n",
    "    bias='none',  # Unsloth supports any, but = 'none' is optimized\n",
    "    # Unsloth does not allow 'all-linear' => manually specify target modules: \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    use_gradient_checkpointing='unsloth',  # True or 'unsloth' for very long context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0353a79e-5eb9-436a-b789-a3c83d1a8870",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = SFTConfig(\n",
    "    output_dir='output/unsloth-phi-3.5-mini-instruct-guanaco',\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True, # Gradient checkpointing improves memory efficiency, but slows down training,\n",
    "        # e.g. Mistral 7B with PEFT using bitsandbytes:\n",
    "        # - enabled: 11 GB GPU RAM and 8 samples/second\n",
    "        # - disabled: 40 GB GPU RAM and 12 samples/second\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},  # Use newer implementation that will become the default.\n",
    "    optim='adamw_torch',\n",
    "    learning_rate=2e-4,  # QLoRA suggestions: 2e-4 for 7B or 13B, 1e-4 for 33B or 65B\n",
    "    logging_strategy='steps',  # 'no', 'epoch' or 'steps'\n",
    "    logging_steps=10,\n",
    "    save_strategy='no',  # 'no', 'epoch' or 'steps'\n",
    "    # save_steps=2000,\n",
    "    # num_train_epochs=5,\n",
    "    max_steps=100,\n",
    "    bf16=True,  # mixed precision training\n",
    "    report_to='none',  # disable wandb\n",
    "    max_seq_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31cff35a-37d2-41c2-ae86-b4ec5e164dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(entry):\n",
    "    return tokenizer.apply_chat_template(entry['messages'], tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "719e9d65-236e-4c2c-a58c-401ddf5fbdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4753bce2790d40e197210878d6baee1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset (num_proc=64):   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32026ec0a3e4d6880b3e183cc70ce12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML (num_proc=64):   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577d1736657844449d70e5885f40a865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=64):   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf14356f5f74f99ba7aa07e08598347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=64):   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0511b38b3ca640c4baf5a4d1dfdac229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset (num_proc=64):   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=guanaco_train,\n",
    "    processing_class=tokenizer,\n",
    "    formatting_func=formatting_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ab99044-63b2-4679-91bb-87ca62e0b83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 9,846 | Num Epochs = 1 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 29,884,416/2,039,024,640 (1.47% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:09, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.444800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.216400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.285300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.101200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.131300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result:\n",
      "TrainOutput(global_step=100, training_loss=1.2025622177124022, metrics={'train_runtime': 132.2607, 'train_samples_per_second': 6.049, 'train_steps_per_second': 0.756, 'total_flos': 1.230171564294144e+16, 'train_loss': 1.2025622177124022})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['UNSLOTH_RETURN_LOGITS'] = '1'\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(\"Training result:\")\n",
    "print(train_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "909731ac-8057-4ff6-a0a5-46bfb9731797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shut down the kernel to release memory\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f34e4a-2f3f-44d4-b3b4-91b53cd6253f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
